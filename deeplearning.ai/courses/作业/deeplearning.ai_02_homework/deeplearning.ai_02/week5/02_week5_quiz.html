<html>
<head>
  <title>02_week5_quiz</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="Evernote Windows/304720 (zh-CN, DDL); Windows/10.0.15063 (Win64);"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="1579"/>
<h1>02_week5_quiz</h1>

<div>
<span><div style="-evernote-webclip:true"><br/><div style="font-size: 16px; display: inline-block;"><div><div style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;"><div><div><div><div><div><div><div><div><div><div><div><div><div><div></div><div><h1 style="font-weight:normal;">Practical aspects of deep learning</h1><div>测验, <span>10 个问题</span></div></div><div style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;"></div></div></div></div><div></div><div></div><div></div><div></div><div><div><div><div><div><div><span>第 1 个问题</span><div>1<br/>point</div><div><h2><span>1。</span><span>第 1 个问题</span></h2><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">If you have 10,000,000 examples, how would you split the train/dev/test set?</p><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;"></p></div></div></span></div><div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">98% train . 1% dev . 1% test</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">33% train .  33% dev . 33% test</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">60% train . 20% dev . 20% test</p></div></div></span></div></span></div></div></div></div></div><div><div><span>第 2 个问题</span><div>1<br/>point</div><div><h2><span>2。</span><span>第 2 个问题</span></h2><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">The dev and test set should:</p><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;"></p><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;"></p></div></div></span></div><div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Come from the same distribution</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Come from different distributions</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Be identical to each other (same (x,y) pairs) </p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;"><strong>Have the same number of examples </strong></p></div></div></span></div></span></div></div></div></div></div><div><div><span>第 3 个问题</span><div>1<br/>point</div><div><h2><span>3。</span><span>第 3 个问题</span></h2><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">If your Neural Network model seems to have high variance, what of the following would be promising things to try?</p></div></div></span></div><div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Add regularization</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Make the Neural Network deeper</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Increase the number of units in each hidden layer </p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Get more test data </p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Get more training data </p></div></div></span></div></span></div></div></div></div></div><div><div><span>第 4 个问题</span><div>1<br/>point</div><div><h2><span>4。</span><span>第 4 个问题</span></h2><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">You are working on an automated check-out kiosk for a supermarket, and are building a classifier for apples, bananas and oranges. Suppose your classifier obtains a training set error of 0.5%, and a dev set error of 7%. Which of the following are promising things to try to improve your classifier? (Check all that apply.)  </p></div></div></span></div><div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Increase the regularization parameter lambda</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Decrease the regularization parameter lambda</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Get more training data </p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Use a bigger neural network</p></div></div></span></div></span></div></div></div></div></div><div><div><span>第 5 个问题</span><div>1<br/>point</div><div><h2><span>5。</span><span>第 5 个问题</span></h2><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">What is weight decay?</p></div></div></span></div><div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">A technique to avoid vanishing gradient by imposing a ceiling on the values of the weights.</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">A regularization technique (such as L2 regularization) that results in gradient descent shrinking the weights on every iteration.</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">The process of gradually decreasing the learning rate during training. </p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Gradual corruption of the weights in the neural network if it is trained on noisy data. </p></div></div></span></div></span></div></div></div></div></div><div><div><span>第 6 个问题</span><div>1<br/>point</div><div><h2><span>6。</span><span>第 6 个问题</span></h2><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">What happens when you increase the regularization hyperparameter lambda?</p></div></div></span></div><div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Weights are pushed toward becoming smaller (closer to 0) </p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Weights are pushed toward becoming bigger (further from 0)</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Doubling lambda should roughly result in doubling the weights</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Gradient descent taking bigger steps with each iteration (proportional to lambda)</p></div></div></span></div></span></div></div></div></div></div><div><div><span>第 7 个问题</span><div>1<br/>point</div><div><h2><span>7。</span><span>第 7 个问题</span></h2><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">With the inverted dropout technique, at test time:</p></div></div></span></div><div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">You apply dropout (randomly eliminating units) and do not keep the 1/keep_prob factor in the calculations used in training</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">You do not apply dropout (do not randomly eliminate units) and do not keep the 1/keep_prob factor in the calculations used in training</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">You do not apply dropout (do not randomly eliminate units), but keep the 1/keep_prob factor in the calculations used in training.</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">You apply dropout (randomly eliminating units) but keep the 1/keep_prob factor in the calculations used in training.</p></div></div></span></div></span></div></div></div></div></div><div><div><span>第 8 个问题</span><div>1<br/>point</div><div><h2><span>8。</span><span>第 8 个问题</span></h2><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Increasing the parameter keep_prob from (say) 0.5 to 0.6 will likely cause the following: (Check the two that apply) </p></div></div></span></div><div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Increasing the regularization effect</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Reducing the regularization effect</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Causing the neural network to end up with a higher training set error </p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Causing the neural network to end up with a lower training set error</p></div></div></span></div></span></div></div></div></div></div><div><div><span>第 9 个问题</span><div>1<br/>point</div><div><h2><span>9。</span><span>第 9 个问题</span></h2><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Which of these techniques are useful for reducing variance (reducing overfitting)? (Check all that apply.)</p></div></div></span></div><div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Xavier initialization</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Gradient Checking</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Exploding gradient</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Vanishing gradient</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Dropout</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">L2 regularization</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Data augmentation</p></div></div></span></div></span></div></div></div></div></div><div><div><span>第 10 个问题</span><div>1<br/>point</div><div><h2><span>10。</span><span>第 10 个问题</span></h2><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Why do we normalize the inputs <span style="color:rgb(136, 136, 136);"></span><span style="font-style:normal;font-weight:normal;line-height:normal;font-size:114%;font-size-adjust:none;text-indent:0px;text-align:left;text-transform:none;letter-spacing:normal;word-spacing:normal;overflow-wrap:normal;white-space:nowrap;float:none;direction:ltr;border:0px none;padding:0px;margin:0px;display:inline;"><span style="position:relative;display:inline-block;white-space:nowrap;"><span style="display:inline-block;width:9px;"><span><span><span>x</span></span></span></span></span></span>?</p></div></div></span></div><div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">Normalization is another word for regularization--It helps to reduce variance  </p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">It makes it easier to visualize the data</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">It makes the parameter initialization faster</p></div></div></span></div></span></div></div><div><div><span><div><span><div><div><p style="font-size:14px;line-height:21px;font-family:&quot;OpenSans&quot;, Arial, sans-serif;">It makes the cost function faster to optimize</p></div></div></span></div></span></div></div></div></div></div></div><div><div><div><div><div><div><span>我了解不是我自己完成的作业将永远不会通过该课程且我的 Coursera 帐号会被取消激活。 </span><span></span></div><div><div></div></div></div></div></div></div></div></div></div><div><div></div><div></div></div><div><div><div><div><div><div><div></div><span></span></div></div><div><div><div></div><span></span></div></div><div><div><div></div><span></span></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><br/></div></span>
</div></body></html> 